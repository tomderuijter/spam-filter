{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Go to source root\n",
    "import os\n",
    "os.chdir('../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1966 / 1966 emails (1.274s).\n",
      "Processed 13705 / 13719 emails (23.198s).\n"
     ]
    }
   ],
   "source": [
    "from email_ingestion import EmailIngester\n",
    "\n",
    "ham_path = '../data/raw/ham/beck-s/'\n",
    "spam_path = '../data/raw/spam/GP/'\n",
    "\n",
    "ingester = EmailIngester(tokenize=False)  # No tokenization. Will use sklearn.\n",
    "ham_data = ingester.ingest_folder(ham_path)\n",
    "spam_data = ingester.ingest_folder(spam_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_label(data, is_spam=False):\n",
    "    for record in data:\n",
    "        record['is_spam'] = is_spam\n",
    "        \n",
    "add_label(ham_data, is_spam=False)\n",
    "add_label(spam_data, is_spam=True)\n",
    "email_data = ham_data + spam_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus indexing\n",
    "Before generating bag of words features, the entire corpus of tokens needs to be indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_corpus(data):\n",
    "    word_counts = count_words(data)\n",
    "    \n",
    "    filter_infrequent_words(word_counts, threshold=5)\n",
    "    filter_long_words(word_counts, threshold=20)\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "\n",
    "def count_words(data):\n",
    "    word_counts = {}\n",
    "\n",
    "    for record in spam_data:\n",
    "\n",
    "        if 'tokens' not in record:\n",
    "            continue\n",
    "\n",
    "        for token in record['tokens']:\n",
    "            if token in word_counts:\n",
    "                word_counts[token] += 1\n",
    "            else:\n",
    "                word_counts[token] = 1\n",
    "    return word_counts    \n",
    "\n",
    "\n",
    "def filter_infrequent_words(word_counts, threshold=5):\n",
    "    infrequent_words = []\n",
    "    \n",
    "    for word in word_counts:\n",
    "        if word_counts[word] < threshold:\n",
    "            infrequent_words.append(word)\n",
    "            \n",
    "    for word in infrequent_words:\n",
    "        word_counts.pop(word)\n",
    "        \n",
    "def filter_long_words(word_counts, threshold=20):\n",
    "    long_words = []\n",
    "    \n",
    "    for word in word_counts:\n",
    "        if len(word) > threshold:\n",
    "            long_words.append(word)\n",
    "            \n",
    "    for word in long_words:\n",
    "        word_counts.pop(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characteristics of unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# TODO No longer works b/c tokenization\n",
    "corpus = generate_corpus(email_data)\n",
    "\n",
    "word_freq = pd.Series(corpus)\n",
    "word_freq.hist(bins=range(5, 50))\n",
    "plt.title('Word frequencies')\n",
    "plt.show()\n",
    "\n",
    "word_len = pd.Series(map(lambda s: len(s), list(corpus.keys())))\n",
    "word_len.hist(bins=range(3, 20))\n",
    "plt.title(\"Word lenght\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Corpus length:\", len(corpus))\n",
    "word_freq.sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual feature generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "# Generate dataset\n",
    "text_data = []\n",
    "for email in email_data:\n",
    "    record = ''\n",
    "    if email['subject'] is not None:\n",
    "        record += email['subject'] + '\\n'\n",
    "    if 'body' in email:\n",
    "        record += email['body']\n",
    "\n",
    "    text_data.append(record)\n",
    "\n",
    "# Generate labels\n",
    "y = []\n",
    "for email in email_data:\n",
    "    y.append(email['is_spam'])\n",
    "\n",
    "# Split dataset\n",
    "# TODO Split on HAM date, not random.\n",
    "# TODO Generate proper training data\n",
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_test, y_train, y_test = train_test_split(text_data, y, test_size=0.33, random_state=42)\n",
    "\n",
    "# Generate features\n",
    "vectorizer = HashingVectorizer(\n",
    "    stop_words='english', non_negative=True,\n",
    "    n_features=2**16\n",
    ")\n",
    "X_train = vectorizer.transform(data_train)\n",
    "X_test = vectorizer.transform(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "train time:   1.970s\n",
      "test time:    0.045s\n",
      "Spam recall:  0.998\n",
      "Ham recall:   0.914\n",
      "\n",
      "classification report:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        HAM       0.99      0.91      0.95       636\n",
      "       SPAM       0.99      1.00      0.99      4536\n",
      "\n",
      "avg / total       0.99      0.99      0.99      5172\n",
      "\n",
      "\n",
      "confusion matrix:\n",
      "[[ 581   55]\n",
      " [   7 4529]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time:   %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:    %0.3fs\" % test_time)\n",
    "\n",
    "    spam_score = metrics.recall_score(y_test, pred, pos_label=True)\n",
    "    ham_score = metrics.recall_score(y_test, pred, pos_label=False)\n",
    "    print(\"Spam recall:  %0.3f\" % spam_score)\n",
    "    print(\"Ham recall:   %0.3f\" % ham_score)\n",
    "    print()\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "            print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "            print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred, target_names=(\"HAM\", \"SPAM\")))\n",
    "    print()\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "    print()\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.utils.extmath import density\n",
    "\n",
    "def benchmark(clf):\n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    if hasattr(clf, 'coef_'):\n",
    "        print(\"dimensionality: %d\" % clf.coef_.shape[1])\n",
    "        print(\"density: %f\" % density(clf.coef_))\n",
    "\n",
    "        if feature_names is not None:\n",
    "            print(\"top 10 keywords per class:\")\n",
    "            for i, label in enumerate(target_names):\n",
    "                top10 = np.argsort(clf.coef_[i])[-10:]\n",
    "                print(trim(\"%s: %s\" % (label, \" \".join(feature_names[top10]))))\n",
    "        print()\n",
    "\n",
    "    print(\"classification report:\")\n",
    "    print(metrics.classification_report(y_test, pred))\n",
    "    print()\n",
    "\n",
    "    print(\"confusion matrix:\")\n",
    "    print(metrics.confusion_matrix(y_test, pred))\n",
    "    print()\n",
    "    \n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "#     return clf_descr, score, train_time, test_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "results = []\n",
    "for clf, name in (\n",
    "        (RidgeClassifier(tol=1e-2, solver=\"lsqr\"), \"Ridge Classifier\"),\n",
    "        (Perceptron(n_iter=50), \"Perceptron\"),\n",
    "        (PassiveAggressiveClassifier(n_iter=50), \"Passive-Aggressive\"),\n",
    "        (KNeighborsClassifier(n_neighbors=10), \"kNN\"),\n",
    "        (RandomForestClassifier(n_estimators=100), \"Random forest\")):\n",
    "    print('=' * 80)\n",
    "    print(name)\n",
    "    results.append(benchmark(clf))\n",
    "\n",
    "for penalty in [\"l2\", \"l1\"]:\n",
    "    print('=' * 80)\n",
    "    print(\"%s penalty\" % penalty.upper())\n",
    "    # Train Liblinear model\n",
    "    results.append(benchmark(LinearSVC(loss='l2', penalty=penalty,\n",
    "                                            dual=False, tol=1e-3)))\n",
    "\n",
    "    # Train SGD model\n",
    "    results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                           penalty=penalty)))\n",
    "\n",
    "# Train SGD with Elastic Net penalty\n",
    "print('=' * 80)\n",
    "print(\"Elastic-Net penalty\")\n",
    "results.append(benchmark(SGDClassifier(alpha=.0001, n_iter=50,\n",
    "                                       penalty=\"elasticnet\")))\n",
    "\n",
    "# Train NearestCentroid without threshold\n",
    "print('=' * 80)\n",
    "print(\"NearestCentroid (aka Rocchio classifier)\")\n",
    "results.append(benchmark(NearestCentroid()))\n",
    "\n",
    "# Train sparse Naive Bayes classifiers\n",
    "print('=' * 80)\n",
    "print(\"Naive Bayes\")\n",
    "results.append(benchmark(MultinomialNB(alpha=.01)))\n",
    "results.append(benchmark(BernoulliNB(alpha=.01)))\n",
    "\n",
    "print('=' * 80)\n",
    "print(\"LinearSVC with L1-based feature selection\")\n",
    "# The smaller C, the stronger the regularization.\n",
    "# The more regularization, the more sparsity.\n",
    "results.append(benchmark(Pipeline([\n",
    "  ('feature_selection', LinearSVC(penalty=\"l1\", dual=False, tol=1e-3)),\n",
    "  ('classification', LinearSVC())\n",
    "])))\n",
    "\n",
    "# make some plots\n",
    "\n",
    "indices = np.arange(len(results))\n",
    "\n",
    "results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "clf_names, score, training_time, test_time = results\n",
    "training_time = np.array(training_time) / np.max(training_time)\n",
    "test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.title(\"Score\")\n",
    "plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "         color='c')\n",
    "plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "plt.yticks(())\n",
    "plt.legend(loc='best')\n",
    "plt.subplots_adjust(left=.25)\n",
    "plt.subplots_adjust(top=.95)\n",
    "plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "for i, c in zip(indices, clf_names):\n",
    "    plt.text(-.3, i, c)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
